# ğŸŒ Dynamic ETL Pipeline for Unstructured Data (MongoDB)

This project is a **universal, format-agnostic ETL pipeline** that processes unstructured files and converts them into clean, structured records stored in MongoDB.

The system:

* Accepts multiple unstructured file formats
* Detects duplicates using hashing
* Extracts relevant content
* Transforms nested structures into flattened JSON documents
* Loads transformed records into MongoDB
* Tracks schemas over time
* Archives original uploaded files
* Provides a simple Streamlit UI for uploading, processing, and deleting files

The goal is to simulate a **real, flexible ETL pipeline** that adapts to unpredictable incoming data.

---

## ğŸš€ Features (What the Project ACTUALLY Does)

### âœ” Supports Multiple File Types

* JSON
* CSV
* XML
* HTML
* TXT / LOG
* Unknown formats â†’ handled as plain text

### âœ” Dynamic Schema Tracking

* Every file is parsed and converted to Python dicts
* The system automatically infers field names and data types
* Schema differences are detected with DeepDiff
* Each new schema version is stored in MongoDB

### âœ” Duplicate File Detection

* Each uploaded file is hashed (SHA-256)
* If the same file is uploaded again â†’ **it is skipped**

### âœ” Flattening of Nested Data

All parsed records are normalized into flat keyâ€“value pairs so MongoDB can store them efficiently.

**Example:**

```
{
  "user": {
    "name": "Alice",
    "address": { "city": "Mumbai", "pin": 400001 }
  }
}
```

**Flattened:**

```
{
  "user.name": "Alice",
  "user.address.city": "Mumbai",
  "user.address.pin": 400001
}
```

### âœ” MongoDB Storage

Each cleaned record is inserted into MongoDB with `_file_hash` so deletion is possible later.

### âœ” File Archiving

Every uploaded file is saved in:

```
archive/<hash>_<filename>
```

### âœ” Web Interface

The Streamlit UI supports:

* File upload
* Multi-file processing
* Summary table
* CSV export
* Delete-by-hash input

---

## ğŸ§© Project Structure

```
Dynamic-ETL-Pipeline/
â”‚
â”œâ”€â”€ app.py                    # Streamlit web interface (main entry point)
â”œâ”€â”€ etl_pipeline.py           # Core ETL logic (Extract, Transform, Load)
â”œâ”€â”€ schema_registry.py        # Schema versioning and tracking
â”œâ”€â”€ config.py                 # Configuration management with dotenv
â”œâ”€â”€ utils.py                  # Helper functions (hashing, date, safe values)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 
â”‚
â”œâ”€â”€ parsers/                  # Format-specific parsers
â”‚   â”œâ”€â”€ json_parser.py        # JSON â†’ dict/list
â”‚   â”œâ”€â”€ csv_parser.py         # CSV â†’ records (via pandas)
â”‚   â”œâ”€â”€ xml_parser.py         # XML â†’ nested dicts (via lxml)
â”‚   â”œâ”€â”€ html_parser.py        # HTML â†’ structured content (via BeautifulSoup)
â”‚   â”œâ”€â”€ text_parser.py        # TXT â†’ raw text with metadata
â”‚   â””â”€â”€ __pycache__/          # Python bytecode cache
â”‚
â”œâ”€â”€ sample_data/              # Example files for testing
â”‚   â”œâ”€â”€ sample.json
â”‚   â”œâ”€â”€ sample.csv
â”‚   â”œâ”€â”€ sample.xml
â”‚   â”œâ”€â”€ sample.html
â”‚   â””â”€â”€ sample.txt
â”‚
â”œâ”€â”€ archive/                  # Processed files archived by SHA-256 hash
â”‚   â””â”€â”€ <hash>_<filename>     # Auto-generated during processing
â”‚
â”œâ”€â”€ venv/                     # Virtual environment (if created)
â”œâ”€â”€ __pycache__/              # Python bytecode cache
â”œâ”€â”€ .git/                     # Git repository data
â””â”€â”€ .gitattributes            # Git configuration
```

---

## ğŸ“¤ Example Outputs

### ğŸŸ¢ First Upload

```
sample.json: File processed successfully (Inserted: 2)
sample.csv: File processed successfully (Inserted: 2)
sample.xml: File processed successfully (Inserted: 1)
sample.html: File processed successfully (Inserted: 1)
sample.txt: File processed successfully (Inserted: 1)
```

### Summary Table

| filename    | hash            | inserted | message                     |
| ----------- | --------------- | -------- | --------------------------- |
| sample.json | 81a9fbb7a91c98â€¦ | 2        | File processed successfully |
| sample.csv  | c0b2e9d2df72e2â€¦ | 2        | File processed successfully |
| sample.xml  | 3d09e6ca812cfbâ€¦ | 1        | File processed successfully |
| sample.html | 4b7c912e6c847fâ€¦ | 1        | File processed successfully |
| sample.txt  | 1a6f30dfd9377aâ€¦ | 1        | File processed successfully |

---

## ğŸ” Duplicate Upload Example

```
sample.json: Duplicate file detected â†’ Skipped ETL. (Inserted: 0)
sample.csv: Duplicate file detected â†’ Skipped ETL. (Inserted: 0)
sample.xml: Duplicate file detected â†’ Skipped ETL. (Inserted: 0)
sample.html: Duplicate file detected â†’ Skipped ETL. (Inserted: 0)
sample.txt: Duplicate file detected â†’ Skipped ETL. (Inserted: 0)
```

MongoDB stays cleanâ€”no duplicates.

---

## ğŸ—‘ File Deletion Example

**Input hash:**

```
90a229a47a1cfb...
```

**Output:**

```
Deleted Records: 2
Registry Removed: 1
Archive Deleted: True
```

---

## ğŸ§  Why This Project Is Useful

* Handles unpredictable unstructured files
* Creates structured, queryable MongoDB data
* Detects and prevents duplicates
* Tracks schema evolution
* Good learning project for ETL and data pipelines

---

## ğŸ‘¨â€ğŸ’» Author

**Swapnil Patil**

## ğŸ‘¨â€ğŸ’» Members of the Team

**Harsh Rai**

**Ekansh Melwani**
